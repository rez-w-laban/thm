{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Supervised Learning on Titanic (2 Hours)\n",
    "\n",
    "Welcome to this 2-hour workshop that integrates our previous notebooks into one cohesive flow, using the Titanic dataset as a real-world example. We'll cover:\n",
    "\n",
    "1. **Preprocessing**:\n",
    "   - **Scaling**: StandardScaler, MinMaxScaler, RobustScaler, and a mention of normalization\n",
    "   - **Encoding**: Ordinal, Target, OneHot, LabelEncoder\n",
    "   - Summaries & comparison tables for each\n",
    "2. **Embeddings**:\n",
    "   - Text features with Bag-of-Words/TF-IDF\n",
    "   - Dimensionality reduction (PCA vs. t-SNE) for visualization\n",
    "   - Mention of advanced word embeddings (Word2Vec)\n",
    "3. **Optimization Techniques**:\n",
    "   - Compare logistic regression solvers (lbfgs, sag) & SGDClassifier\n",
    "   - Hyperparameter tuning: GridSearchCV vs. RandomizedSearchCV\n",
    "4. **Evaluation Metrics**:\n",
    "   - Accuracy, Precision, Recall, confusion matrix\n",
    "5. **Plotting & Comparison**:\n",
    "   - Confusion matrix heatmap\n",
    "   - Bar chart of model performance\n",
    "\n",
    "We'll highlight why we might choose each scaler or encoder, incorporate text columns, demonstrate embedding visuals, and compare optimization strategies. Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Cleaning the Titanic Dataset\n",
    "\n",
    "We load Titanic from seaborn. This dataset includes numeric, categorical, and a textual column (embark_town)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "titanic = sns.load_dataset('titanic').copy()\n",
    "print('Initial shape:', titanic.shape)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict 'survived' (0 or 1). We'll remove columns with many NaNs (deck) or partial redundancy (alive, alone, who, adult_male). We'll also drop rows missing age or embarked for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.drop(columns=['deck','alive','alone','who','adult_male'], inplace=True)\n",
    "titanic.dropna(subset=['age','embarked'], inplace=True)\n",
    "print('After cleanup, shape:', titanic.shape)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "### 2.1 Scaling: StandardScaler, MinMaxScaler, RobustScaler, mention Normalization\n",
    "\n",
    "1. **StandardScaler**: transforms each feature to have mean=0, std=1.\n",
    "2. **MinMaxScaler**: rescales features to [0,1].\n",
    "3. **RobustScaler**: uses median and IQR, more robust to outliers.\n",
    "4. **Normalization** (L1 or L2 norm) often used for text or similar.\n",
    "\n",
    "| Scaler         | Pros                                 | Cons                             |\n",
    "|----------------|--------------------------------------|----------------------------------|\n",
    "| StandardScaler | Mean=0, std=1, widely used           | Outliers shift mean/std          |\n",
    "| MinMaxScaler   | Maps to [0,1], intuitive range        | Outliers skew min, max           |\n",
    "| RobustScaler   | Less outlier impact (median, IQR)     | Not in [0,1], no strict bound     |\n",
    "| Normalization  | Good for text features (unit norm)    | Less common for numeric features |\n",
    "\n",
    "We'll pick numeric columns age, sibsp, parch, fare and compare each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "numeric_cols = ['age','sibsp','parch','fare']\n",
    "titanic_num = titanic[numeric_cols].copy()\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "mm_scaler = MinMaxScaler()\n",
    "rb_scaler = RobustScaler()\n",
    "\n",
    "X_std = std_scaler.fit_transform(titanic_num)\n",
    "X_mm  = mm_scaler.fit_transform(titanic_num)\n",
    "X_rb  = rb_scaler.fit_transform(titanic_num)\n",
    "\n",
    "df_std = pd.DataFrame(X_std, columns=numeric_cols)\n",
    "df_mm  = pd.DataFrame(X_mm,  columns=numeric_cols)\n",
    "df_rb  = pd.DataFrame(X_rb,  columns=numeric_cols)\n",
    "\n",
    "print('StandardScaler stats:\\n', df_std.describe().loc[['mean','std','min','max']])\n",
    "print('\\nMinMaxScaler stats:\\n', df_mm.describe().loc[['min','max']])\n",
    "print('\\nRobustScaler stats:\\n', df_rb.describe().loc[['mean','std','min','max']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Encoders: Ordinal, Target, OneHot, Label\n",
    "\n",
    "We'll demonstrate them on sex, embarked, class.\n",
    "\n",
    "| Encoder     | Pros                                         | Cons                                      |\n",
    "|-------------|----------------------------------------------|-------------------------------------------|\n",
    "| Ordinal     | Minimal dimension, single integer per cat    | Implies an order that might not exist     |\n",
    "| Target      | Uses mean of target in each category         | Risk of leakage if not careful            |\n",
    "| OneHot      | Standard, no ordinal assumption              | Dimensions can blow up for large cat sets |\n",
    "| Label       | Good for y or truly ordinal cat              | If used for input, can impose false order |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install category_encoders --quiet\n",
    "from category_encoders import OrdinalEncoder, TargetEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "cat_cols = ['sex','embarked','class']\n",
    "df_cat = titanic[cat_cols].copy()\n",
    "y_surv = titanic['survived'].astype(int).values\n",
    "\n",
    "ord_enc = OrdinalEncoder(cols=cat_cols)\n",
    "df_ord = ord_enc.fit_transform(df_cat, y_surv)\n",
    "\n",
    "tgt_enc = TargetEncoder(cols=cat_cols)\n",
    "df_tgt = tgt_enc.fit_transform(df_cat, y_surv)\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False, drop=None)\n",
    "df_ohe_arr = ohe.fit_transform(df_cat)\n",
    "df_ohe_cols = ohe.get_feature_names_out(cat_cols)\n",
    "df_ohe = pd.DataFrame(df_ohe_arr, columns=df_ohe_cols)\n",
    "\n",
    "lbl_enc = LabelEncoder()\n",
    "sex_le = lbl_enc.fit_transform(df_cat['sex'])\n",
    "\n",
    "print('OrdinalEncoder head:\\n', df_ord.head())\n",
    "print('\\nTargetEncoder head:\\n', df_tgt.head())\n",
    "print('\\nOneHotEncoder head:\\n', df_ohe.head())\n",
    "print('\\nLabelEncoder for sex:', sex_le[:10], '... classes:', lbl_enc.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings\n",
    "\n",
    "We'll treat embark_town as text, demonstrating a simple Bag-of-Words or TF-IDF approach. Then we'll do PCA vs. t-SNE on numeric data.\n",
    "\n",
    "### 3.1 Bag-of-Words / TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "town_data = titanic['embark_town'].fillna('')\n",
    "cv = CountVectorizer()\n",
    "bow_mat = cv.fit_transform(town_data)\n",
    "print('CountVectorizer shape:', bow_mat.shape)\n",
    "print('Vocabulary:', cv.get_feature_names_out())\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_mat = tfidf.fit_transform(town_data)\n",
    "print('TF-IDF shape:', tfidf_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 PCA vs. t-SNE for numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# We'll do PCA and t-SNE on standard-scaled numeric data\n",
    "X_numeric_std = X_std  # from earlier (StandardScaler on age,sibsp,parch,fare)\n",
    "y_survived = titanic['survived'].astype(int).values\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_numeric_std)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_numeric_std)\n",
    "\n",
    "def plot_embed(X_emb, y, title):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.scatter(X_emb[:,0], X_emb[:,1], c=y, cmap='viridis', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label='survived')\n",
    "    plt.show()\n",
    "\n",
    "plot_embed(X_pca, y_survived, 'PCA on Titanic numeric')\n",
    "plot_embed(X_tsne, y_survived, 't-SNE on Titanic numeric')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Advanced Word Embeddings\n",
    "For large textual data, Word2Vec or GloVe can produce dense vectors capturing semantic meaning. Here, embark_town is too trivial. In practice, you'd do something like:\n",
    "```python\n",
    "# from gensim.models import Word2Vec\n",
    "# w2v_model = Word2Vec(list_of_tokenized_sentences, vector_size=100, ...)\n",
    "```\n",
    "Then incorporate word vectors into your ML pipeline. We'll skip the full demonstration here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization Techniques\n",
    "\n",
    "We'll build a pipeline for classification. We'll do a train/test split on the entire dataset, then compare:\n",
    "- LogisticRegression with lbfgs vs. sag\n",
    "- SGDClassifier\n",
    "- RandomForest with GridSearch & RandomizedSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df = titanic.copy()\n",
    "y = train_df['survived'].astype(int)\n",
    "train_df.drop(columns=['survived'], inplace=True)\n",
    "\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(train_df, y, test_size=0.3, random_state=42, stratify=y)\n",
    "print('Train shape:', X_train_df.shape, 'Test shape:', X_test_df.shape)\n",
    "\n",
    "numeric_cols = ['age','sibsp','parch','fare']\n",
    "cat_cols     = ['sex','embarked','class']\n",
    "text_col     = 'embark_town'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 ColumnTransformer Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.key].fillna('').values\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('ohe', OneHotEncoder(drop=None, sparse=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "text_transformer = Pipeline([\n",
    "    ('selector', TextSelector(text_col)),\n",
    "    ('bow', CountVectorizer())\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, cat_cols),\n",
    "    ('text', text_transformer, [text_col])\n",
    "])\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Logistic Regression solvers vs. SGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_lbfgs = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('clf', LogisticRegression(solver='lbfgs', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "lr_sag = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('clf', LogisticRegression(solver='sag', max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "sgd_clf = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('clf', SGDClassifier(loss='log', random_state=42, max_iter=1000, tol=1e-3))\n",
    "])\n",
    "\n",
    "pipelines = {\n",
    "    'LogReg_lbfgs': lr_lbfgs,\n",
    "    'LogReg_sag': lr_sag,\n",
    "    'SGDClassifier': sgd_clf\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, pipe in pipelines.items():\n",
    "    pipe.fit(X_train_df, y_train)\n",
    "    acc = pipe.score(X_test_df, y_test)\n",
    "    results[name] = acc\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 RandomForest: GridSearchCV & RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [50, 100],\n",
    "    'clf__max_depth': [None, 3, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf_pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train_df, y_train)\n",
    "print('GridSearch best params:', grid_search.best_params_)\n",
    "print('GridSearch best CV score:', grid_search.best_score_)\n",
    "rf_grid_best = grid_search.best_estimator_\n",
    "acc_rf_grid = rf_grid_best.score(X_test_df, y_test)\n",
    "print('Test Accuracy (RF GridSearch):', acc_rf_grid)\n",
    "\n",
    "param_dist = {\n",
    "    'clf__n_estimators': randint(10,200),\n",
    "    'clf__max_depth': [None,3,5,7]\n",
    "}\n",
    "\n",
    "rand_search = RandomizedSearchCV(rf_pipeline, param_dist, n_iter=5, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "rand_search.fit(X_train_df, y_train)\n",
    "print('\\nRandomSearch best params:', rand_search.best_params_)\n",
    "print('RandomSearch best CV score:', rand_search.best_score_)\n",
    "rf_rand_best = rand_search.best_estimator_\n",
    "acc_rf_rand = rf_rand_best.score(X_test_df, y_test)\n",
    "print('Test Accuracy (RF RandomSearch):', acc_rf_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Plotting\n",
    "We'll compare these final models in terms of Accuracy, Precision, and Recall, plus a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(name, model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    prec = precision_score(y, y_pred)\n",
    "    rec = recall_score(y, y_pred)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(f'---- {name} ----')\n",
    "    print(f'Accuracy: {acc:.3f}')\n",
    "    print(f'Precision: {prec:.3f}')\n",
    "    print(f'Recall: {rec:.3f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "    return (acc, prec, rec, cm)\n",
    "\n",
    "model_scores = {}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    acc, prec, rec, cm = evaluate_model(name, pipe, X_test_df, y_test)\n",
    "    model_scores[name] = acc\n",
    "\n",
    "acc_rf_g, prec_rf_g, rec_rf_g, cm_rf_g = evaluate_model('RF_Grid', rf_grid_best, X_test_df, y_test)\n",
    "model_scores['RF_Grid'] = acc_rf_g\n",
    "\n",
    "acc_rf_r, prec_rf_r, rec_rf_r, cm_rf_r = evaluate_model('RF_Rand', rf_rand_best, X_test_df, y_test)\n",
    "model_scores['RF_Rand'] = acc_rf_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = max(model_scores, key=model_scores.get)\n",
    "print('Best model by accuracy:', best_model_name, '->', model_scores[best_model_name])\n",
    "\n",
    "all_cm = {\n",
    "    'LogReg_lbfgs': evaluate_model('LogReg_lbfgs', pipelines['LogReg_lbfgs'], X_test_df, y_test)[3],\n",
    "    'LogReg_sag': evaluate_model('LogReg_sag', pipelines['LogReg_sag'], X_test_df, y_test)[3],\n",
    "    'SGDClassifier': evaluate_model('SGDClassifier', pipelines['SGDClassifier'], X_test_df, y_test)[3],\n",
    "    'RF_Grid': cm_rf_g,\n",
    "    'RF_Rand': cm_rf_r\n",
    "}\n",
    "\n",
    "best_cm = all_cm[best_model_name]\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(best_cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title(f'{best_model_name} - Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Bar Chart of Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(model_scores.keys())\n",
    "accs = [model_scores[n] for n in names]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "bars = plt.bar(names, accs, color=['orange','green','purple','blue','red'])\n",
    "plt.ylim([0,1])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height()+0.01,\n",
    "             f'{bar.get_height():.3f}', ha='center', va='bottom')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison Tables & Final Takeaways\n",
    "\n",
    "### 6.1 Scaling Methods\n",
    "\n",
    "| Scaler         | Pros                                  | Cons                             |\n",
    "|----------------|---------------------------------------|----------------------------------|\n",
    "| StandardScaler | Mean=0, std=1, widely used            | Outliers shift mean/std          |\n",
    "| MinMaxScaler   | Maps to [0,1], intuitive range         | Outliers skew min, max           |\n",
    "| RobustScaler   | Less outlier impact (median, IQR)      | Not in [0,1], no strict bound     |\n",
    "| Normalization  | Good for text features (unit norm)     | Less common for numeric features |\n",
    "\n",
    "### 6.2 Encoding Methods\n",
    "\n",
    "| Encoder     | Pros                                   | Cons                                                          |\n",
    "|-------------|----------------------------------------|---------------------------------------------------------------|\n",
    "| Ordinal     | Minimal dimension, single integer cat   | Implies ordering that might not exist                         |\n",
    "| Target      | Uses mean of target in each category    | Potential leakage if not cross-validation aware               |\n",
    "| OneHot      | Standard, no ordinal assumption         | Dimension blow-up for high-cardinality features              |\n",
    "| Label       | Good for labeling y or truly ordinal    | If used for input, might incorrectly impose numeric ordering |\n",
    "\n",
    "### 6.3 Embeddings\n",
    "\n",
    "| Method       | Description                                   | Use Case                                                 |\n",
    "|--------------|-----------------------------------------------|----------------------------------------------------------|\n",
    "| Bag-of-Words / TF-IDF | Basic textual representation, token counts/frequencies  | Smaller text fields, easy to interpret                   |\n",
    "| Word2Vec / GloVe  | Dense vectors capturing semantic meaning  | Larger text corpora, advanced NLP tasks                 |\n",
    "| PCA         | Linear dimension reduction for numeric data     | Quick embedding, captures variance linearly             |\n",
    "| t-SNE       | Nonlinear, preserves local distances            | Clearer cluster visuals, but can be slow or tricky to tune |\n",
    "\n",
    "### 6.4 Optimization / Tuning\n",
    "\n",
    "| Approach             | Pros                                    | Cons                                        |\n",
    "|----------------------|-----------------------------------------|---------------------------------------------|\n",
    "| SGDClassifier        | Can handle large data in streaming mode | Needs careful LR scheduling, sensitive      |\n",
    "| lbfgs / sag (LogReg) | Often stable for moderate data sizes     | Memory-heavy if data is huge               |\n",
    "| GridSearchCV         | Exhaustive param search                 | Slow if param space is large               |\n",
    "| RandomizedSearchCV   | Samples param space for speed           | Might skip best combos                     |\n",
    "\n",
    "## Final Observations\n",
    "- The best model often depends on the dataset's nature. RandomForest might do well if tuned.\n",
    "- For text, advanced embeddings (Word2Vec) can help. For small text fields, one-hot or bag-of-words might suffice.\n",
    "- Always consider outliers when choosing a scaler.\n",
    "- Check cardinality when choosing an encoder.\n",
    "- Evaluate with cross-validation, especially if using target encoding.\n",
    "\n",
    "### End of 2-Hour Integrated Notebook\n",
    "We have combined the basics from previous notebooks with the Titanic dataset, demonstrating scaling (Standard, MinMax, Robust), encoding (Ordinal, Target, OneHot, mention Label), text embeddings (Bag-of-Words, mention Word2Vec), numeric embeddings (PCA vs. t-SNE), optimization (SGD, logistic solvers, random forest tuning), and evaluation (accuracy, precision, recall) with plots.\n",
    "\n",
    "Thank you for following this integrated session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('End of the integrated 2-hour Titanic supervised learning notebook. Thank you!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
