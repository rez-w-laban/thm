{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrEhwEKQkKWk",
        "outputId": "64f09901-7c33-492d-fee5-1762cb5c9e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "N=1000 | y∈[0,1] | S(min,max) among known=0.000/1.000\n",
            "Observed (train) pairs=44955 | Holdout (pairs)=4995 | Unobserved=449550\n",
            "#Atoms found: 30\n",
            "Fingerprint shape: (1000, 264)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-663452729.py:361: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  print(f\"[C-simple] ep {ep:02d}  train_loss={float(loss):.6f}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[C-simple] ep 01  train_loss=0.063342\n",
            "[C-simple holdout] MSE=0.061842  MAE=0.227510  Pearson=0.010  Spearman=0.006\n",
            "[C-simple] ep 02  train_loss=0.061117\n",
            "[C-simple holdout] MSE=0.059270  MAE=0.222276  Pearson=0.010  Spearman=0.007\n",
            "[C-simple] ep 03  train_loss=0.058208\n",
            "[C-simple holdout] MSE=0.055594  MAE=0.214556  Pearson=0.012  Spearman=0.008\n",
            "[C-simple] ep 05  train_loss=0.049389\n",
            "[C-simple holdout] MSE=0.044370  MAE=0.189520  Pearson=0.020  Spearman=0.013\n",
            "[C-simple] ep 10  train_loss=0.021382\n",
            "[C-simple holdout] MSE=0.022428  MAE=0.126556  Pearson=0.047  Spearman=0.027\n",
            "[C-simple] ep 20  train_loss=0.016593\n",
            "[C-simple holdout] MSE=0.016327  MAE=0.097882  Pearson=0.086  Spearman=0.046\n",
            "[C-simple] ep 40  train_loss=0.012738\n",
            "[C-simple holdout] MSE=0.013168  MAE=0.077652  Pearson=0.184  Spearman=0.117\n",
            "[C-simple] ep 300  train_loss=0.003181\n",
            "[C-simple holdout] MSE=0.003436  MAE=0.037350  Pearson=0.867  Spearman=0.782\n",
            "[C-simple FINAL holdout] MSE=0.003436  MAE=0.037350  Pearson=0.867  Spearman=0.782\n",
            "Teacher Z shape: (1000, 128)\n",
            "[Teacher sanity] S≈ZZ^T: MSE=0.0234  Pearson=0.992  Spearman=0.989\n",
            "Split sizes: train=900 val=33 test=67\n",
            "[Student] ep 01  train_loss=0.04641  val_pair_Pearson=0.671  Spearman=0.599\n",
            "[Student] ep 02  train_loss=0.02505  val_pair_Pearson=0.793  Spearman=0.643\n",
            "[Student] ep 03  train_loss=0.01398  val_pair_Pearson=0.781  Spearman=0.676\n",
            "[Student] ep 05  train_loss=0.01001  val_pair_Pearson=0.843  Spearman=0.721\n",
            "[Student] ep 10  train_loss=0.00552  val_pair_Pearson=0.891  Spearman=0.781\n",
            "[Student] ep 20  train_loss=0.00368  val_pair_Pearson=0.936  Spearman=0.833\n",
            "[Student] ep 40  train_loss=0.00268  val_pair_Pearson=0.951  Spearman=0.867\n",
            "[Aligned student] pairwise cosine Pearson=0.951  Spearman=0.868\n",
            "[MLP on Z_pred] ep 01  val_auc=0.481  val_ap=0.448\n",
            "[MLP on Z_pred] ep 02  val_auc=0.608  val_ap=0.512\n",
            "[MLP on Z_pred] ep 03  val_auc=0.677  val_ap=0.582\n",
            "[MLP on Z_pred] ep 05  val_auc=0.762  val_ap=0.680\n",
            "[MLP on Z_pred] ep 10  val_auc=0.865  val_ap=0.817\n",
            "[MLP on Z_pred] ep 15  val_auc=0.931  val_ap=0.894\n",
            "[MLP on Z_pred] ep 20  val_auc=0.962  val_ap=0.929\n",
            "\n",
            "[MLP on Z_pred] τ*=0.642 | TEST AUC=0.970  AP=0.965  F1=0.894  Acc=0.925\n",
            "[[41  1]\n",
            " [ 4 21]]\n",
            "Saved: S_hat_Csimple.csv, Z_teacher_from_Csimple.csv, Z_student_pred.csv, semantic_features.csv\n"
          ]
        }
      ],
      "source": [
        "# ================== ONE CELL: C-simple + FP→Z + Truth ==================\n",
        "# If needed (Colab): !pip -q install numpy pandas scipy scikit-learn torch\n",
        "import os, re, math, random, hashlib, numpy as np, pandas as pd\n",
        "from numpy.linalg import eigh\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score, confusion_matrix, mean_squared_error, mean_absolute_error\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# ---------------- CONFIG ----------------\n",
        "CSV_PATH           = \"/content/30_1000_base.csv\"  # first column y, then N formula columns (S)\n",
        "SEED               = 7\n",
        "DIM                = 128               # embedding dim for spectral Z and student\n",
        "M_PROBES           = 256               # # of semantic mini-worlds (fingerprint)\n",
        "OBS_FRACTION       = 0.10              # used only if CSV has no missing entries (to simulate partial observation)\n",
        "HOLDOUT_FRACTION   = 0.10              # fraction of observed pairs kept for leak-free eval of completion\n",
        "PAIR_EVAL          = 20000             # #pairs to sample for correlation sanity checks\n",
        "# GNN (C-simple)\n",
        "EPOCHS_C           = 300\n",
        "BATCH_EDGES_C      = 40000\n",
        "LR_C               = 1e-3\n",
        "APPNP_K            = 10\n",
        "APPNP_ALPHA        = 0.1\n",
        "EDGE_TEMP          = 1.0               # keep 1.0 for pure regression; 1.5–2.0 to sharpen propagation later\n",
        "BLOCK_PRED         = 128               # block size for full kernel prediction\n",
        "# Student (FP→Z)\n",
        "EPOCHS_STUDENT     = 40\n",
        "LR_STUDENT         = 2e-3\n",
        "PAIR_SAMPLES       = 2048              # pairwise cosine samples per batch\n",
        "PAIR_LOSS_W        = 0.5               # weight for pairwise loss vs vector MSE\n",
        "BATCH_STUDENT      = 512\n",
        "# Truth head\n",
        "EPOCHS_TRUTH       = 20\n",
        "LR_TRUTH           = 1e-3\n",
        "BATCH_TRUTH        = 512\n",
        "# Saving\n",
        "SAVE_ARTIFACTS     = True\n",
        "OUT_DIR            = \"./\"\n",
        "\n",
        "# ---------------- Repro & Device ----------------\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------------- Load CSV ----------------\n",
        "assert os.path.exists(CSV_PATH), f\"File not found: {CSV_PATH}\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "N  = df.shape[0]\n",
        "formulas = list(df.columns[1:])\n",
        "assert len(formulas) == N, \"Expected N rows and N formula columns (1..N).\"\n",
        "y = df.iloc[:, 0].to_numpy().astype(np.float32)\n",
        "S_raw = df.iloc[:, 1:].to_numpy(dtype=float)  # may include NaN for unobserved pairs\n",
        "\n",
        "# Ensure symmetry (even if sparse) but keep NaNs where both missing\n",
        "S = 0.5*(S_raw + S_raw.T)\n",
        "diag = np.eye(N, dtype=bool)\n",
        "S[diag] = 1.0\n",
        "# Clip known entries into [0,1]\n",
        "mask_known = ~np.isnan(S)\n",
        "S[mask_known] = np.clip(S[mask_known], 0.0, 1.0)\n",
        "y_bin = (y > 0.5).astype(np.float32)\n",
        "print(f\"N={N} | y∈[0,1] | S(min,max) among known={np.nanmin(S):.3f}/{np.nanmax(S):.3f}\")\n",
        "\n",
        "# ---------------- Build observed/unobserved/holdout pairs ----------------\n",
        "def upper_pairs(N):\n",
        "    # return all (i,j) with i<j\n",
        "    I, J = np.triu_indices(N, k=1)\n",
        "    return np.stack([I, J], axis=1)\n",
        "\n",
        "all_pairs_u = upper_pairs(N)\n",
        "\n",
        "# If the CSV is dense (no NaNs), simulate a partial observation set\n",
        "if np.isnan(S).sum() == 0:\n",
        "    total = len(all_pairs_u)\n",
        "    m_obs = max(1, int(OBS_FRACTION * total))\n",
        "    idx = np.random.RandomState(SEED).choice(total, size=m_obs, replace=False)\n",
        "    obs_pairs_all = all_pairs_u[idx]\n",
        "    obs_vals_all  = S[obs_pairs_all[:,0], obs_pairs_all[:,1]]\n",
        "else:\n",
        "    # Use only pairs with known values (i<j)\n",
        "    Kmask = (~np.isnan(S)) & (~np.eye(N, dtype=bool))\n",
        "    I, J = np.where(np.triu(Kmask, k=1))\n",
        "    obs_pairs_all = np.stack([I, J], axis=1)\n",
        "    obs_vals_all  = S[I, J].astype(np.float32)\n",
        "\n",
        "# Split observed into train-observed vs holdout-observed for leak-free completion eval\n",
        "rng = np.random.RandomState(SEED)\n",
        "perm = rng.permutation(len(obs_pairs_all))\n",
        "m_hold = max(1, int(HOLDOUT_FRACTION * len(obs_pairs_all)))\n",
        "hold_idx = perm[:m_hold]; train_obs_idx = perm[m_hold:]\n",
        "holdout_pairs = obs_pairs_all[hold_idx]\n",
        "holdout_true  = obs_vals_all[hold_idx]\n",
        "obs_pairs     = obs_pairs_all[train_obs_idx]\n",
        "obs_vals      = obs_vals_all[train_obs_idx]\n",
        "\n",
        "# Unobserved = all remaining upper pairs not in obs_pairs_all\n",
        "obs_set = set(map(tuple, obs_pairs_all.tolist()))\n",
        "unobs_pairs = np.array([p for p in all_pairs_u.tolist() if tuple(p) not in obs_set], dtype=np.int64)\n",
        "\n",
        "print(f\"Observed (train) pairs={len(obs_pairs)} | Holdout (pairs)={len(holdout_pairs)} | Unobserved={len(unobs_pairs)}\")\n",
        "\n",
        "# ---------------- Propositional Parser & Robust FP (unseen atoms handled) ----------------\n",
        "OP_MAP = {\"→\":\" IMP \", \"⇒\":\" IMP \", \"=>\":\" IMP \", \"->\":\" IMP \",\n",
        "          \"↔\":\" IFF \", \"<=>\":\" IFF \", \"<->\":\" IFF \",\n",
        "          \"⊑\":\" SUB \",  # treat as IMP\n",
        "          \"⊓\":\" AND \", \"∧\":\" AND \", \"&&\":\" AND \",\n",
        "          \"⊔\":\" OR  \", \"∨\":\" OR  \", \"||\":\" OR  \",\n",
        "          \"¬\":\" NOT \", \"~\":\" NOT \", \"!\":\" NOT \"}\n",
        "BIN_OPS, UNARY_OPS = {\"AND\",\"OR\",\"IMP\",\"IFF\",\"SUB\"}, {\"NOT\"}\n",
        "TOKEN_RE = re.compile(r\"[A-Za-z0-9_]+|[()]\")\n",
        "\n",
        "def norm_text(s):\n",
        "    s = str(s)\n",
        "    for k,v in OP_MAP.items(): s = s.replace(k,v)\n",
        "    return s\n",
        "\n",
        "def lex(s): return TOKEN_RE.findall(norm_text(s))\n",
        "def is_atom(t): return t not in BIN_OPS|UNARY_OPS|{\"(\",\")\"}\n",
        "\n",
        "class Parser:\n",
        "    def __init__(self,toks): self.toks=toks; self.i=0\n",
        "    def peek(self): return self.toks[self.i] if self.i<len(self.toks) else None\n",
        "    def pop(self): t=self.peek(); self.i += (1 if t is not None else 0); return t\n",
        "    def parse(self): return self.expr(0)\n",
        "    PREC = {\"IFF\":1,\"IMP\":2,\"SUB\":2,\"OR\":3,\"AND\":4}\n",
        "    RIGHT = {\"IMP\",\"IFF\",\"SUB\"}\n",
        "    def expr(self,minp):\n",
        "        node=self.unary()\n",
        "        while True:\n",
        "            op=self.peek()\n",
        "            if op in BIN_OPS:\n",
        "                prec=self.PREC.get(op,0)\n",
        "                if prec<minp: break\n",
        "                self.pop()\n",
        "                nextp = prec if op in self.RIGHT else prec+1\n",
        "                rhs=self.expr(nextp)\n",
        "                node=(\"BIN\",op,node,rhs)\n",
        "            else: break\n",
        "        return node\n",
        "    def unary(self):\n",
        "        t=self.peek()\n",
        "        if t in UNARY_OPS:\n",
        "            self.pop(); c=self.unary(); return (\"UN\",t,c)\n",
        "        if t==\"(\":\n",
        "            self.pop(); n=self.expr(0); assert self.pop()==\")\",\"Missing ')'\"\n",
        "            return n\n",
        "        a=self.pop()\n",
        "        return (\"ATOM\", a if a is not None else \"x\")\n",
        "\n",
        "def parse_formula(s):\n",
        "    try: return Parser(lex(s)).parse()\n",
        "    except: return (\"ATOM\",\"x\")\n",
        "\n",
        "def atoms_in(node, acc=None):\n",
        "    if acc is None: acc=set()\n",
        "    k=node[0]\n",
        "    if k==\"ATOM\": acc.add(node[1]); return acc\n",
        "    if k==\"UN\": return atoms_in(node[2], acc)\n",
        "    if k==\"BIN\": atoms_in(node[2], acc); atoms_in(node[3], acc); return acc\n",
        "    return acc\n",
        "\n",
        "def depth(node):\n",
        "    k=node[0]\n",
        "    if k==\"ATOM\": return 1\n",
        "    if k==\"UN\": return 1+depth(node[2])\n",
        "    if k==\"BIN\": return 1+max(depth(node[2]), depth(node[3]))\n",
        "    return 1\n",
        "\n",
        "# --- Deterministic hashing for unseen atoms (probe-consistent) ---\n",
        "def _u64_from_str(s: str) -> int:\n",
        "    h = hashlib.blake2b(s.encode('utf-8'), digest_size=8).digest()\n",
        "    return int.from_bytes(h, 'big')\n",
        "\n",
        "def bernoulli_from_name(atom: str, probe_idx: int, p: float, seed: int) -> bool:\n",
        "    u = (_u64_from_str(f\"{atom}|{probe_idx}|{seed}\") % (1<<53)) / float(1<<53)\n",
        "    return u < p\n",
        "\n",
        "class ProbeEnv:\n",
        "    def __init__(self, base_env: dict, probe_idx: int, bias_p: float, seed: int):\n",
        "        self.base = base_env\n",
        "        self.m    = probe_idx\n",
        "        self.p    = float(bias_p)\n",
        "        self.seed = int(seed)\n",
        "        self.cache = {}\n",
        "    def get(self, atom: str) -> bool:\n",
        "        if atom in self.base: return bool(self.base[atom])\n",
        "        if atom in self.cache: return self.cache[atom]\n",
        "        v = bernoulli_from_name(atom, self.m, self.p, self.seed)\n",
        "        self.cache[atom] = v\n",
        "        return v\n",
        "\n",
        "def eval_ast(node, env_obj):\n",
        "    k=node[0]\n",
        "    if k==\"ATOM\": return bool(env_obj.get(node[1]))\n",
        "    if k==\"UN\":\n",
        "        _,op,c = node\n",
        "        v = eval_ast(c, env_obj)\n",
        "        return (not v)\n",
        "    if k==\"BIN\":\n",
        "        _,op,l,r = node\n",
        "        a = eval_ast(l, env_obj); b = eval_ast(r, env_obj)\n",
        "        if op==\"AND\": return a and b\n",
        "        if op==\"OR\":  return a or b\n",
        "        if op in (\"IMP\",\"SUB\"): return (not a) or b\n",
        "        if op==\"IFF\": return a==b\n",
        "    return False\n",
        "\n",
        "# ---------------- Build semantic fingerprint (FP) ----------------\n",
        "asts = [parse_formula(s) for s in formulas]\n",
        "all_atoms = sorted(set().union(*[atoms_in(t) for t in asts]))\n",
        "A = len(all_atoms)\n",
        "print(f\"#Atoms found: {A}\")\n",
        "\n",
        "# probes (half bias 0.3, half 0.7)\n",
        "rng = np.random.default_rng(SEED)\n",
        "biases = np.concatenate([np.full(M_PROBES//2, 0.3), np.full(M_PROBES - M_PROBES//2, 0.7)])\n",
        "assignments = []\n",
        "for p in biases:\n",
        "    vals = rng.random(A) < p\n",
        "    env = {a: bool(v) for a,v in zip(all_atoms, vals)}  # known atoms only\n",
        "    assignments.append(env)\n",
        "\n",
        "# Truth matrix T: N x M_PROBES with unseen atoms handled via ProbeEnv\n",
        "T_mat = np.zeros((N, M_PROBES), dtype=np.float32)\n",
        "for i,ast in enumerate(asts):\n",
        "    for m_i,base_env in enumerate(assignments):\n",
        "        env_obj = ProbeEnv(base_env, probe_idx=m_i, bias_p=biases[m_i], seed=SEED)\n",
        "        T_mat[i,m_i] = 1.0 if eval_ast(ast, env_obj) else 0.0\n",
        "\n",
        "# Structural features\n",
        "def op_counts(toks):\n",
        "    return toks.count(\"AND\"), toks.count(\"OR\"), toks.count(\"NOT\"), toks.count(\"IMP\")+toks.count(\"SUB\"), toks.count(\"IFF\")\n",
        "struct_rows=[]\n",
        "for s,ast in zip(formulas,asts):\n",
        "    toks = lex(s)\n",
        "    ac = len(atoms_in(ast, set()))\n",
        "    d  = depth(ast)\n",
        "    c_and, c_or, c_not, c_imp, c_iff = op_counts(toks)\n",
        "    struct_rows.append([ac, d, c_and, c_or, c_not, c_imp, c_iff, len(toks)])\n",
        "STRUCT = np.array(struct_rows, dtype=np.float32)\n",
        "\n",
        "# Final FP\n",
        "FP = np.concatenate([T_mat, STRUCT], axis=1).astype(np.float32)\n",
        "print(\"Fingerprint shape:\", FP.shape)\n",
        "\n",
        "# ---------------- FP preprocessing for models ----------------\n",
        "# For GNN encoder features (lowered dimension for stability)\n",
        "sc_fp_gnn = StandardScaler().fit(FP)\n",
        "FP_std_g  = sc_fp_gnn.transform(FP).astype(np.float32)\n",
        "pca_gnn   = PCA(n_components=min(256, FP_std_g.shape[1]), whiten=True, random_state=SEED).fit(FP_std_g)\n",
        "FP_low    = pca_gnn.transform(FP_std_g).astype(np.float32)\n",
        "\n",
        "# For student (we'll standardize inside the student trainer using train only)\n",
        "# ---------------- Pairwise metrics ----------------\n",
        "def evaluate_pairs(truth, pred):\n",
        "    mse = float(mean_squared_error(truth, pred))\n",
        "    mae = float(mean_absolute_error(truth, pred))\n",
        "    if len(truth) > 1:\n",
        "        pe = float(pearsonr(truth, pred)[0])\n",
        "        sp = float(spearmanr(truth, pred)[0])\n",
        "    else:\n",
        "        pe = sp = float('nan')\n",
        "    return {\"mse\":mse, \"mae\":mae, \"pearson\":pe, \"spearman\":sp}\n",
        "\n",
        "def print_metrics(name, m):\n",
        "    print(f\"[{name}] MSE={m['mse']:.6f}  MAE={m['mae']:.6f}  Pearson={m['pearson']:.3f}  Spearman={m['spearman']:.3f}\")\n",
        "\n",
        "# ---------------- C-simple: APPNP encoder + calibrated dot decoder ----------------\n",
        "def build_A_hat_from_obs(N, pairs_idx, weights, edge_temp=1.0):\n",
        "    w = np.clip(weights.astype(np.float32), 0.0, 1.0)\n",
        "    if edge_temp != 1.0:\n",
        "        w = np.exp(edge_temp * w)\n",
        "    rows = np.concatenate([pairs_idx[:,0], pairs_idx[:,1]])\n",
        "    cols = np.concatenate([pairs_idx[:,1], pairs_idx[:,0]])\n",
        "    vals = np.concatenate([w, w])\n",
        "    # self-loops\n",
        "    rows = np.concatenate([rows, np.arange(N)])\n",
        "    cols = np.concatenate([cols, np.arange(N)])\n",
        "    vals = np.concatenate([vals, np.ones(N, dtype=np.float32)])\n",
        "    A = sp.coo_matrix((vals, (rows, cols)), shape=(N, N)).tocsr()\n",
        "    d = np.array(A.sum(1)).ravel()\n",
        "    d_inv_sqrt = 1.0 / np.sqrt(np.maximum(d, 1e-8))\n",
        "    D_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    A_hat = (D_inv_sqrt @ A @ D_inv_sqrt).tocoo()\n",
        "    idx = np.vstack([A_hat.row, A_hat.col])\n",
        "    val = A_hat.data.astype(np.float32)\n",
        "    i = torch.tensor(idx, dtype=torch.long, device=device)\n",
        "    v = torch.tensor(val, dtype=torch.float32, device=device)\n",
        "    return torch.sparse_coo_tensor(i, v, (N, N), device=device)\n",
        "\n",
        "class APPNPEncoder(nn.Module):\n",
        "    def __init__(self, d_in, d_hidden, d_out, p_drop=0.1):\n",
        "        super().__init__()\n",
        "        self.lin1 = nn.Linear(d_in, d_hidden)\n",
        "        self.lin2 = nn.Linear(d_hidden, d_out)\n",
        "        self.drop = nn.Dropout(p_drop)\n",
        "    def forward(self, X, A_hat, K=10, alpha=0.1):\n",
        "        H0 = F.relu(self.lin1(X))\n",
        "        H0 = self.drop(H0)\n",
        "        H0 = self.lin2(H0)\n",
        "        H  = H0\n",
        "        for _ in range(K):\n",
        "            H = (1 - alpha) * torch.sparse.mm(A_hat, H) + alpha * H0\n",
        "        return H\n",
        "\n",
        "class GNNCompletion(nn.Module):\n",
        "    def __init__(self, d_in, d_hidden, d_out):\n",
        "        super().__init__()\n",
        "        self.enc = APPNPEncoder(d_in, d_hidden, d_out, p_drop=0.1)\n",
        "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
        "        self.bias  = nn.Parameter(torch.tensor(0.0))\n",
        "    def forward_embeddings(self, X, A_hat, K, alpha):\n",
        "        Z = self.enc(X, A_hat, K=K, alpha=alpha)\n",
        "        with torch.no_grad():\n",
        "            norms = Z.norm(dim=1, keepdim=True).clamp_min(1e-6)\n",
        "            Z /= norms.clamp_max(2.0)\n",
        "        return F.normalize(Z, dim=1)\n",
        "    def decode_pairs(self, Z, pairs_idx):\n",
        "        i = torch.as_tensor(pairs_idx[:,0], dtype=torch.long, device=Z.device)\n",
        "        j = torch.as_tensor(pairs_idx[:,1], dtype=torch.long, device=Z.device)\n",
        "        dp = (Z[i] * Z[j]).sum(1)\n",
        "        return torch.sigmoid(self.scale * dp + self.bias)\n",
        "\n",
        "def train_gnn_completion_simple(\n",
        "    FP_low, obs_pairs_idx, obs_vals,\n",
        "    holdout_pairs_idx=None, holdout_true=None,\n",
        "    rank_d=128, d_hidden=128, epochs=60, batch_pairs=40000, lr=1e-3,\n",
        "    appnp_K=10, appnp_alpha=0.1, edge_temp=1.0\n",
        "):\n",
        "    N, d_in = FP_low.shape\n",
        "    X  = torch.tensor(FP_low, dtype=torch.float32, device=device)\n",
        "    Ah = build_A_hat_from_obs(N, obs_pairs_idx, obs_vals, edge_temp=edge_temp)\n",
        "    model = GNNCompletion(d_in, d_hidden, rank_d).to(device)\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        Z = model.forward_embeddings(X, Ah, K=appnp_K, alpha=appnp_alpha)\n",
        "        idx = np.random.randint(0, len(obs_pairs_idx), size=min(batch_pairs, len(obs_pairs_idx)))\n",
        "        batch = obs_pairs_idx[idx]\n",
        "        s_t   = torch.as_tensor(obs_vals[idx], dtype=torch.float32, device=device)\n",
        "        pred = model.decode_pairs(Z, batch).clamp(0,1)\n",
        "        loss = F.mse_loss(pred, s_t)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 2.0)\n",
        "        opt.step()\n",
        "\n",
        "        if ep in {1,2,3,5,10,20,40,epochs} and (holdout_pairs_idx is not None):\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                Z_eval = model.forward_embeddings(X, Ah, K=appnp_K, alpha=appnp_alpha)\n",
        "                i = torch.as_tensor(holdout_pairs_idx[:,0], dtype=torch.long, device=device)\n",
        "                j = torch.as_tensor(holdout_pairs_idx[:,1], dtype=torch.long, device=device)\n",
        "                ph = torch.sigmoid(model.scale * (Z_eval[i]*Z_eval[j]).sum(1) + model.bias).clamp(0,1).cpu().numpy()\n",
        "                m  = evaluate_pairs(holdout_true, ph)\n",
        "            print(f\"[C-simple] ep {ep:02d}  train_loss={float(loss):.6f}\")\n",
        "            print_metrics(\"C-simple holdout\", m)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        Z = model.forward_embeddings(torch.tensor(FP_low, dtype=torch.float32, device=device), Ah, K=appnp_K, alpha=appnp_alpha)\n",
        "    return model, Z\n",
        "\n",
        "def predict_full_kernel_from_Z(model, Z, block=128):\n",
        "    N = Z.shape[0]\n",
        "    K = np.zeros((N, N), dtype=np.float32)\n",
        "    for i0 in range(0, N, block):\n",
        "        i1 = min(N, i0+block)\n",
        "        Ii = torch.arange(i0, i1, device=Z.device)\n",
        "        Zi = Z[Ii]\n",
        "        for j0 in range(0, N, block):\n",
        "            j1 = min(N, j0+block)\n",
        "            Jj = torch.arange(j0, j1, device=Z.device)\n",
        "            Zj = Z[Jj]\n",
        "            with torch.no_grad():\n",
        "                dp = Zi @ Zj.T\n",
        "                pred = torch.sigmoid(model.scale * dp + model.bias).clamp(0,1).cpu().numpy()\n",
        "            K[i0:i1, j0:j1] = pred\n",
        "    K = 0.5*(K + K.T); np.fill_diagonal(K, 1.0)\n",
        "    return K\n",
        "\n",
        "# ---- Train C-simple and evaluate on HOLDOUT observed pairs only (no leakage) ----\n",
        "model_C, Z_C, = train_gnn_completion_simple(\n",
        "    FP_low, obs_pairs, obs_vals,\n",
        "    holdout_pairs_idx=holdout_pairs, holdout_true=holdout_true,\n",
        "    rank_d=DIM, d_hidden=DIM, epochs=EPOCHS_C, batch_pairs=BATCH_EDGES_C, lr=LR_C,\n",
        "    appnp_K=APPNP_K, appnp_alpha=APPNP_ALPHA, edge_temp=EDGE_TEMP\n",
        ")\n",
        "\n",
        "S_hat_Csimple = predict_full_kernel_from_Z(model_C, Z_C, block=BLOCK_PRED)\n",
        "preds_hold = S_hat_Csimple[holdout_pairs[:,0], holdout_pairs[:,1]]\n",
        "m_Csimple = evaluate_pairs(holdout_true, preds_hold)\n",
        "print_metrics(\"C-simple FINAL holdout\", m_Csimple)\n",
        "\n",
        "# ---------------- Spectral Embedding (Teacher Z from completed kernel) ----------------\n",
        "def spectral_embedding_psd(K: np.ndarray, d: int) -> np.ndarray:\n",
        "    K = 0.5*(K + K.T)\n",
        "    vals, vecs = eigh(K)\n",
        "    idx = np.argsort(vals)[::-1]\n",
        "    vals, vecs = vals[idx], vecs[:, idx]\n",
        "    vals = np.clip(vals, 0.0, None)\n",
        "    keep = min(d, max(1, int((vals > 1e-9).sum())))\n",
        "    U = vecs[:, :keep]; L = np.sqrt(vals[:keep])\n",
        "    Z = U * L\n",
        "    Z = Z / (np.linalg.norm(Z, axis=1, keepdims=True) + 1e-12)\n",
        "    if keep < d:\n",
        "        Zp = np.zeros((K.shape[0], d), dtype=Z.dtype); Zp[:, :keep] = Z; Z = Zp\n",
        "    return Z.astype(np.float32)\n",
        "\n",
        "Z_teacher = spectral_embedding_psd(S_hat_Csimple, DIM)\n",
        "print(\"Teacher Z shape:\", Z_teacher.shape)\n",
        "# Teacher sanity on random pairs\n",
        "m = min(PAIR_EVAL, N*N)\n",
        "ii = np.random.randint(0, N, size=m); jj = np.random.randint(0, N, size=m)\n",
        "S_hat = (Z_teacher[ii] * Z_teacher[jj]).sum(1)\n",
        "S_ij  = S_hat_Csimple[ii, jj]\n",
        "print(\"[Teacher sanity] S≈ZZ^T: MSE=%.4f  Pearson=%.3f  Spearman=%.3f\" %\n",
        "      (float(((S_hat - S_ij)**2).mean()), pearsonr(S_ij, S_hat)[0], spearmanr(S_ij, S_hat)[0]))\n",
        "\n",
        "# ---------------- Train/Val/Test split for TRUTH (labels used ONLY here) ----------------\n",
        "idx_all = np.arange(N)\n",
        "train_idx, temp_idx = train_test_split(idx_all, test_size=0.1, random_state=SEED, stratify=y_bin)\n",
        "val_idx,   test_idx = train_test_split(temp_idx, test_size=2/3, random_state=SEED, stratify=y_bin[temp_idx])\n",
        "print(f\"Split sizes: train={len(train_idx)} val={len(val_idx)} test={len(test_idx)}\")\n",
        "\n",
        "# ---------------- Student: FP -> Z_teacher (align on TRAIN only) ----------------\n",
        "# Standardize FP for student using TRAIN ONLY\n",
        "sc_student = StandardScaler().fit(FP[train_idx])\n",
        "FP_std_all = sc_student.transform(FP).astype(np.float32)\n",
        "\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.f = nn.Sequential(\n",
        "            nn.Linear(d_in, 512), nn.ReLU(),\n",
        "            nn.Linear(512, 256), nn.ReLU(),\n",
        "            nn.Linear(256, d_out)\n",
        "        )\n",
        "    def forward(self, x): return self.f(x)\n",
        "\n",
        "X_all_t   = torch.tensor(FP_std_all, dtype=torch.float32, device=device)\n",
        "Z_teach_t = torch.tensor(Z_teacher,  dtype=torch.float32, device=device)\n",
        "student   = StudentNet(FP_std_all.shape[1], DIM).to(device)\n",
        "opt_s     = torch.optim.Adam(student.parameters(), lr=LR_STUDENT)\n",
        "\n",
        "def train_student():\n",
        "    idxs = np.arange(N)\n",
        "    for ep in range(1, EPOCHS_STUDENT+1):\n",
        "        np.random.shuffle(idxs)\n",
        "        tot=0; n=0\n",
        "        for i in range(0, N, BATCH_STUDENT):\n",
        "            sb = idxs[i:i+BATCH_STUDENT]\n",
        "            xb = X_all_t[sb]\n",
        "            zt = Z_teach_t[sb]\n",
        "            zhat = student(xb)\n",
        "\n",
        "            # vector loss (MSE in embedding space)\n",
        "            loss_vec = F.mse_loss(zhat, zt)\n",
        "\n",
        "            # pairwise cosine loss on sampled pairs from this mini-batch\n",
        "            with torch.no_grad():\n",
        "                b = len(sb)\n",
        "                p = min(PAIR_SAMPLES, b*b)\n",
        "                ii = torch.randint(0, b, (p,), device=device)\n",
        "                jj = torch.randint(0, b, (p,), device=device)\n",
        "                t_i = F.normalize(zt,   dim=1)[ii]\n",
        "                t_j = F.normalize(zt,   dim=1)[jj]\n",
        "                cos_t = (t_i * t_j).sum(1)\n",
        "\n",
        "            p_i = F.normalize(zhat, dim=1)[ii]\n",
        "            p_j = F.normalize(zhat, dim=1)[jj]\n",
        "            cos_p = (p_i * p_j).sum(1)\n",
        "            loss_pair = F.mse_loss(cos_p, cos_t)\n",
        "\n",
        "            loss = loss_vec + PAIR_LOSS_W*loss_pair\n",
        "            opt_s.zero_grad(); loss.backward(); opt_s.step()\n",
        "            tot += float(loss.item())*len(sb); n += len(sb)\n",
        "\n",
        "        if ep in {1,2,3,5,10,20,EPOCHS_STUDENT}:\n",
        "            with torch.no_grad():\n",
        "                zhat_all = F.normalize(student(X_all_t), dim=1)\n",
        "                zt_all   = F.normalize(Z_teach_t, dim=1)\n",
        "                m = min(PAIR_EVAL, N*N)\n",
        "                ii = torch.randint(0, N, (m,), device=device)\n",
        "                jj = torch.randint(0, N, (m,), device=device)\n",
        "                cos_p = (zhat_all[ii] * zhat_all[jj]).sum(1).cpu().numpy()\n",
        "                cos_t = (zt_all[ii]   * zt_all[jj]).sum(1).cpu().numpy()\n",
        "                pr = pearsonr(cos_t, cos_p)[0]; sp = spearmanr(cos_t, cos_p)[0]\n",
        "            print(f\"[Student] ep {ep:02d}  train_loss={tot/max(1,n):.5f}  val_pair_Pearson={pr:.3f}  Spearman={sp:.3f}\")\n",
        "\n",
        "train_student()\n",
        "\n",
        "# Predicted Z (normalize), then Procrustes align on TRAIN ONLY\n",
        "with torch.no_grad():\n",
        "    Z_pred0 = student(X_all_t).cpu().numpy().astype(np.float32)\n",
        "Z_pred0 /= (np.linalg.norm(Z_pred0, axis=1, keepdims=True) + 1e-12)\n",
        "# Orthogonal Procrustes using TRAIN ONLY\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "Q, _ = orthogonal_procrustes(Z_pred0[train_idx], Z_teacher[train_idx])\n",
        "Z_pred = (Z_pred0 @ Q)\n",
        "Z_pred /= (np.linalg.norm(Z_pred, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "# Geometry sanity\n",
        "m = min(PAIR_EVAL, N*N)\n",
        "ii = np.random.randint(0, N, size=m); jj = np.random.randint(0, N, size=m)\n",
        "cos_pred = np.sum(Z_pred[ii]*Z_pred[jj], axis=1)\n",
        "cos_teac = np.sum(Z_teacher[ii]*Z_teacher[jj], axis=1)\n",
        "print(\"[Aligned student] pairwise cosine Pearson=%.3f  Spearman=%.3f\" %\n",
        "      (pearsonr(cos_teac, cos_pred)[0], spearmanr(cos_teac, cos_pred)[0]))\n",
        "\n",
        "# ---------------- Truth MLP on Z_pred (train/val/test) ----------------\n",
        "class TruthMLP(nn.Module):\n",
        "    def __init__(self, d_in, d_h=64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(nn.Linear(d_in, d_h), nn.ReLU(), nn.Linear(d_h, 1))\n",
        "    def forward(self, x): return self.net(x).squeeze(1)\n",
        "\n",
        "def train_eval_truth_mlp(name, X, y_bin, train_idx, val_idx, test_idx, epochs=EPOCHS_TRUTH):\n",
        "    sc = StandardScaler().fit(X[train_idx])\n",
        "    X_all  = sc.transform(X).astype(np.float32)\n",
        "    X_all_t= torch.tensor(X_all, dtype=torch.float32, device=device)\n",
        "    y_all_t= torch.tensor(y_bin, dtype=torch.float32, device=device)\n",
        "\n",
        "    n_pos = int(y_bin[train_idx].sum()); n_neg = int(len(train_idx) - n_pos)\n",
        "    pos_weight = torch.tensor(max(1.0, n_neg / max(1, n_pos)), dtype=torch.float32, device=device)\n",
        "    bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    model = TruthMLP(X.shape[1], 64).to(device)\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=LR_TRUTH)\n",
        "\n",
        "    def iterate(idxs, train=True, batch=BATCH_TRUTH):\n",
        "        model.train(mode=train); tot=0; n=0\n",
        "        for i in range(0, len(idxs), batch):\n",
        "            sb = idxs[i:i+batch]\n",
        "            xb = X_all_t[sb]; yb = y_all_t[sb]\n",
        "            loss = bce(model(xb), yb)\n",
        "            if train: opt.zero_grad(set_to_none=True); loss.backward(); opt.step()\n",
        "            tot += float(loss.item())*len(sb); n += len(sb)\n",
        "        return tot/max(1,n)\n",
        "\n",
        "    def probs(idxs):\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            return torch.sigmoid(model(X_all_t[idxs])).cpu().numpy()\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        _ = iterate(train_idx, True)\n",
        "        if ep in {1,2,3,5,10,15,20}:\n",
        "            pv = probs(val_idx)\n",
        "            print(f\"[{name}] ep {ep:02d}  val_auc={roc_auc_score(y_bin[val_idx], pv):.3f}  val_ap={average_precision_score(y_bin[val_idx], pv):.3f}\")\n",
        "\n",
        "    # tune τ on VAL\n",
        "    pv_val = probs(val_idx)\n",
        "    ts = np.linspace(0,1,501); best_t=0.5; best_f1=-1\n",
        "    for t in ts:\n",
        "        f1 = f1_score(y_bin[val_idx], (pv_val>=t).astype(int), zero_division=0)\n",
        "        if f1>best_f1: best_f1=f1; best_t=t\n",
        "\n",
        "    pv_test = probs(test_idx); yhat=(pv_test>=best_t).astype(int)\n",
        "    auc = roc_auc_score(y_bin[test_idx], pv_test)\n",
        "    ap  = average_precision_score(y_bin[test_idx], pv_test)\n",
        "    f1  = f1_score(y_bin[test_idx], yhat)\n",
        "    acc = accuracy_score(y_bin[test_idx], yhat)\n",
        "    cm  = confusion_matrix(y_bin[test_idx], yhat)\n",
        "    print(f\"\\n[{name}] τ*={best_t:.3f} | TEST AUC={auc:.3f}  AP={ap:.3f}  F1={f1:.3f}  Acc={acc:.3f}\")\n",
        "    print(cm)\n",
        "    return {\"name\":name,\"auc\":auc,\"ap\":ap,\"f1\":f1,\"acc\":acc,\"tau\":best_t,\"probs_all\":None}\n",
        "\n",
        "# Run the truth head on Z_pred\n",
        "res_Zp = train_eval_truth_mlp(\"MLP on Z_pred\", Z_pred, y_bin, train_idx, val_idx, test_idx)\n",
        "\n",
        "# ---------------- Save artifacts (optional) ----------------\n",
        "if SAVE_ARTIFACTS:\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "    pd.DataFrame(S_hat_Csimple, index=formulas, columns=formulas).to_csv(os.path.join(OUT_DIR, \"S_hat_Csimple.csv\"))\n",
        "    pd.DataFrame(Z_teacher, index=formulas).to_csv(os.path.join(OUT_DIR, \"Z_teacher_from_Csimple.csv\"))\n",
        "    pd.DataFrame(Z_pred,    index=formulas).to_csv(os.path.join(OUT_DIR, \"Z_student_pred.csv\"))\n",
        "    pd.DataFrame(FP,        index=formulas).to_csv(os.path.join(OUT_DIR, \"semantic_features.csv\"))\n",
        "    print(\"Saved: S_hat_Csimple.csv, Z_teacher_from_Csimple.csv, Z_student_pred.csv, semantic_features.csv\")\n",
        "# ======================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3yz7DC2k0rA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}